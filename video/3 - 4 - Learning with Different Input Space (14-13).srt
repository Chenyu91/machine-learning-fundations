1
00:00:00,000 --> 00:00:06,220
好，最後的一個部分呢，我們現在來跟大家講。我們剛才講的大部分都跟

2
00:00:06,220 --> 00:00:12,440
輸出有關，就是說我們說今天輸出的空間有變化的時候，有什麼樣不同的機器學習

3
00:00:12,440 --> 00:00:18,765
問題。今天輸出的這個標記，我們拿到的標記資料有什麼不同的時候，有怎麼樣機器學習

4
00:00:18,765 --> 00:00:25,090
問題。今天我們取得這些標記的方法或取得資料的方法有不同的時候，有什麼樣問題？

5
00:00:25,090 --> 00:00:30,900
那最後我們要來談談說輸入 x 的部分有些什麼樣不同的變化？

6
00:00:30,900 --> 00:00:38,085
那我們之前想像實際上，我們之前想說比如說信用卡這個問題，那我們就喂給機器什麼？信用卡

7
00:00:38,085 --> 00:00:45,270
的申請書，然後這個申請書我們要准還是不要准，輸入的部分就是這些申請書上填的資料

8
00:00:45,270 --> 00:00:52,720
然後這些資料來看的話，我們對這些資料假設大體上是說這些資料非常的具體

9
00:00:52,720 --> 00:00:57,800
concrete 非常的具體，而且它跟我們想要做的事情可能有一些些關係

10
00:00:57,800 --> 00:01:05,010
更進一步來說，實際上它們代表一些蠻複雜的已經經過處理的資訊，我們相信這些資訊

11
00:01:05,010 --> 00:01:10,450
可以跟我們想要輸出有關係，我們通常叫這個叫做 concrete

12
00:01:10,450 --> 00:01:16,010
那輸入部分，有時候叫 input，有時候我們常常會把它叫 feature

13
00:01:16,010 --> 00:01:22,190
特徵 也就是說，我們這些輸入特徵的部分到底跟我的輸出有什麼關係呢？

14
00:01:22,190 --> 00:01:29,245
concrete 我們之前想的就是 concrete
的時候，我們可以就是說，好，那這些既然是 concrete，我們可能可以用這些

15
00:01:29,245 --> 00:01:35,439
這個具體的東西來算一個總分，就像我們在上一次提過的這個線性 linear

16
00:01:35,439 --> 00:01:40,850
model linear hypothesis 這個
H 做的事情就是，這些事情很 concrete

17
00:01:40,850 --> 00:01:46,111
那我就來把它們加一加，算個總分，然後再看看我要不要給信用卡 那這個

18
00:01:46,111 --> 00:01:53,230
concrete 我們之前其實一直都大體上是在這樣的想像上面做

19
00:01:53,230 --> 00:02:01,060
事情，像說，我們說要做銅板的分類，那可能我們用的就是它的大小還有它的重量

20
00:02:01,060 --> 00:02:10,310
那或者是說，我們要做信用卡，信用卡要不要發的這件事情，我們就
說我們用的是具體的顧客資料。我們做癌症，我們就是我們用的是具體的

21
00:02:10,310 --> 00:02:19,061
病人的資料。通常這些資料裏面，這些特徵裏面都帶有
人類的智慧對這個問題的描述，我們常常稱之為

22
00:02:19,061 --> 00:02:28,270
domain knowledge，也就是說你對這個問題的專業知識
所以這是把人類的專業知識放在，也就是我們經過一些

23
00:02:28,270 --> 00:02:38,020
有點像預處理的，腦袋裏思考預處理的動作，才會繼續去做
那這些對機器學習來說，大家可以想像是比較簡單的問題。說簡單

24
00:02:38,020 --> 00:02:43,000
其實不簡單，但是相對於我們底下要提到的，大家會發現這是比較簡單的。

25
00:02:43,000 --> 00:02:48,190
什麼意思呢？我們再來想另外一個問題，如果我們今天

26
00:02:48,190 --> 00:02:57,930
要做剛才講的手寫數字的辨識，像什麼辨識，郵遞區號好了
好，比如說這些是人寫下來數字，那我們可以把這些

27
00:02:57,930 --> 00:03:02,480
數字還有它們相對的意義，喂給機器學習，這是一個 batch

28
00:03:02,480 --> 00:03:10,250
multi
class，多類別、批次的這樣的問題喂給機器，然後我們就可以得到一個自動的

29
00:03:10,250 --> 00:03:16,070
數字的辨識程式。好，這是一個典型的問題。但是我問大家

30
00:03:16,070 --> 00:03:21,920
如果你要做這個問題的話，你會喂給機器什麼當作你的 x

31
00:03:21,920 --> 00:03:26,000
？ 一個可能性是這樣

32
00:03:26,000 --> 00:03:32,500
我們說，好吧，我們要做數字的辨識啊，那我可能可以從我這些數字，寫下來的數字裏面

33
00:03:32,500 --> 00:03:38,350
去推出，或思考一下哪些東西跟我的辨識有關

34
00:03:38,350 --> 00:03:46,657
那這裡我是舉一個例子說，例如說今天這個數字寫的到底對不對稱？
還有呢，今天這個數字的密度怎麼樣?

35
00:03:46,657 --> 00:03:54,810
密度怎麼樣是說，我今天這個格子裏面我們到底
是筆劃很複雜，然後占了很多格，黑色的，還是筆劃很簡單，占了很少格黑色的?

36
00:03:54,810 --> 00:04:02,657
什麼意思？我們看看，如果今天稍微 簡化一點問題，我說我要分 1 或者是

37
00:04:02,657 --> 00:04:09,120
5 好了; 1 有什麼特性？1 比 5 來的對稱，對吧？

38
00:04:09,120 --> 00:04:14,377
好，那 1 還有什麼特性，1 比 5 的這個 density 密度

39
00:04:14,377 --> 00:04:21,440
來的低 所以呢如果我們直的代表對不對稱，1 會在比較高的地方，5 會在比較低的地方

40
00:04:21,440 --> 00:04:26,505
同樣的如果我們這個橫軸代表密度的話，1 會在

41
00:04:26,505 --> 00:04:31,570
這個比較左邊，density 比較低的地方，5
會在右邊，density 比較高的地方

42
00:04:31,570 --> 00:04:37,480
所以這些是，我們就把 1 跟 5 轉化轉化，變成我們現在已經很熟悉的二維

43
00:04:37,480 --> 00:04:43,390
平面上這個圖形，我們會看到一堆 1
我用圈圈代表，在這邊，一堆 5 我用叉叉代表，在這邊

44
00:04:43,390 --> 00:04:48,255
它除非什麼，除非那個字寫的不好，比如說這個其實是一個 5

45
00:04:48,255 --> 00:04:55,768
當然也有可能掃描掃的不好，所以它就會在一些比較不正確的地方 這種是我們的所謂
concrete

46
00:04:55,768 --> 00:05:05,040
feature，非常具體，我們用人對
這個問題的分析去想出來說，好吧，那我們可能就塞這樣的東西給機器來學

47
00:05:05,040 --> 00:05:10,311
另外一種是什麼，有人說很簡單嘛，這個數字就是 16 乘

48
00:05:10,311 --> 00:05:17,965
16， 16 乘 16 就是 256 格，我就把它變成一個，之前好像我們大部分都在

49
00:05:17,965 --> 00:05:22,890
說 x 是一個向量，那我就把它變成一個 256 個維度的向量就好

50
00:05:22,890 --> 00:05:28,360
假如它是灰階的，灰階我可能可以做一個正規化的動作，把它變成 0 到 1 之間的數字

51
00:05:28,360 --> 00:05:35,530
所以我就會有一個，每一個影片，對不起，每一個數字就是一個 256 維的向量在這邊。

52
00:05:35,530 --> 00:05:39,550
好，所以你說這 256 維的向量

53
00:05:39,550 --> 00:05:46,950
每一個維度有沒有物理上的意義？有！ 他覺得很簡單，就代表那一個點而已

54
00:05:46,950 --> 00:05:54,190
如果你要說真正我們寫出來的數字，不是靠一個點，是靠這個點跟旁邊其他點到底有寫沒有寫-
等等的關係

55
00:05:54,190 --> 00:06:00,135
所以這樣 256 維的可能會比我們剛才講那個兩維的

56
00:06:00,135 --> 00:06:06,080
對稱或不對稱，然後濃度 density 怎麼樣，來得更抽象一些

57
00:06:06,080 --> 00:06:12,820
越抽象就表示什麼，越抽象就表示對機器來說這個問題越困難。

58
00:06:12,820 --> 00:06:19,455
好，那個但是這個東西很常見了，你如果在做視覺的辨識，你如果在做聲音的辨識，常常

59
00:06:19,455 --> 00:06:25,285
你拿到的就是聲音的頻譜，或者是這個視覺的每一個 pixel

60
00:06:25,285 --> 00:06:32,370
每一個點 好，這個時候，機器可能要做一些事情，說我要把這個抽象的

61
00:06:32,370 --> 00:06:41,800
轉成它比較 帶有一些真正的特徵，就是具體的來做，這個過程可能是機器自動做的

62
00:06:41,800 --> 00:06:51,260
有可能是人幫著機器做的，人幫著機器做，我們常常把 這個東西取一個很好聽的名字叫做
feature engineering。

63
00:06:51,260 --> 00:06:58,270
這是這個特徵工程，我們要用一個工程學的方法去想說到底我們要喂給機器什麼樣

64
00:06:58,270 --> 00:07:04,562
東西。那機器自動做，也有，大家可能有聽過這幾年很紅的一個東西叫做 deep
learning

65
00:07:04,562 --> 00:07:14,070
深度學習，那它做的事情 非常概略的說，就是它希望有大量的資料，甚至是用非監督的方式

66
00:07:14,070 --> 00:07:22,990
去學我們要怎麼樣從裏面抽取出非常 具體的特徵。那我們這些抽取比較具體特徵的方法

67
00:07:22,990 --> 00:07:29,680
會在下一門課，也就是 machine learning
technics 這門課的後期我們會跟大家提到，

68
00:07:29,680 --> 00:07:36,227
不過這不是太容易的問題。對我們機器學習設定來說沒什麼變化，我們還是有個輸入 這個輸入

69
00:07:36,227 --> 00:07:41,360
256 維的向量沒有什麼變，但是對機器學到東西的難度

70
00:07:41,360 --> 00:07:51,270
通常會變大。好，那這些 還不是最困難的。最困難的是非常非常抽象。

71
00:07:51,270 --> 00:07:55,700
什麼非常非常抽象？大家記得我跟大家提說我們在 KDDCup

72
00:07:55,700 --> 00:08:01,320
2011 年的時候我們要做的事情是預測每個使用者會給每部

73
00:08:01,320 --> 00:08:10,768
歌曲幾分。對吧，所以我們來看，我們現在已經熟悉了，分數是什麼？分數是
我們想要的輸出，如果是

74
00:08:10,768 --> 00:08:15,861
0 到 100 分的話，我們可以說，好 說我們輸出是所有的實數，或者是 0 到

75
00:08:15,861 --> 00:08:22,710
100 之間的這個實數 這是一個典型的
regression 迴歸分析的問題，但輸入是什麼？

76
00:08:22,710 --> 00:08:30,180
輸入我們只有使用者是誰 這首歌是什麼的

77
00:08:30,180 --> 00:08:36,423
ID 號碼，編號，比如說這是 257 號使用者他給 1126

78
00:08:36,423 --> 00:08:43,125
號的歌曲 多少分。輸入二維向量，你說，二維向量聽起來很好啊，可是

79
00:08:43,125 --> 00:08:49,110
這個二維向量，大家可以想像不是我們剛才這個對稱性，然後密度這樣的二維向量

80
00:08:49,110 --> 00:08:54,275
可以比擬的，因為我們現在這些二維的數字

81
00:08:54,275 --> 00:08:59,440
都只是抽象，系統給使用者的一個編號，系統給歌曲的一個編號

82
00:08:59,440 --> 00:09:07,080
你需要怎麼做，所以我之前跟大家提到了，我們必須幫每個使用者抽取出它真正的特徵

83
00:09:07,080 --> 00:09:14,720
你比如說，他到底對這些歌曲的喜好的這個向量，幫每首歌曲抽取出它的特徵

84
00:09:14,720 --> 00:09:24,130
這個歌曲到底曲風是怎麼樣，或者它的作曲家是怎麼樣，等等，把這些特質弄出來
再用這些特徵去學。這些特徵怎麼出來的？好，有一部份

85
00:09:24,130 --> 00:09:29,579
可能是人想的，另外一部份是機器自己去學到 的，所以跟剛才的這個

86
00:09:29,579 --> 00:09:35,445
raw feature 一樣，跟剛才這些 原始的這些
feature 一樣，今天如果你有抽象的

87
00:09:35,445 --> 00:09:42,520
feature 的時候，你 也是需要機器自己去學到真正具體的東西是怎麼樣。

88
00:09:42,520 --> 00:09:47,015
這個東西還蠻常見的，比如說我們在 KDDCup 裏面常常都有說今天

89
00:09:47,015 --> 00:09:51,510
我有使用者的 ID ，然後這些編號到底能夠做什麼

90
00:09:51,510 --> 00:09:58,677
事情，或者我有廣告的編號，廣告編號能做什麼事情，這個就 比剛才講的這個

91
00:09:58,677 --> 00:10:03,185
raw feature 就是原始的 feature 更要再抽像。

92
00:10:03,185 --> 00:10:08,780
可能難度也會更高一些，會需要更多特徵的抽取動作

93
00:10:08,780 --> 00:10:15,095
一個比較對大家，這個，對大家想要至少心裡想的是，越抽象

94
00:10:15,095 --> 00:10:21,410
機器就要花越大的力氣去裡面找出這個比較具體它可以真正拿來做的東西，然後所以

95
00:10:21,410 --> 00:10:30,680
困難度通常就會越高。好，所以總結來說呢 我們這邊跟大家講的這三種不同的輸入的形式。

96
00:10:30,680 --> 00:10:37,968
最，我們其實一直，假設我們有的，都是那種最具體的方式，那如果我們今天，不是那麼具體-

97
00:10:37,968 --> 00:10:43,690
，比如說 我們是原始資料或者是抽象資料怎麼辦？那通常就需要人或者是機器

98
00:10:43,690 --> 00:10:48,700
想辦法去抽取一些具體的特徵出來，那這個我們就會擺在比較進階的部分。

99
00:10:48,700 --> 00:10:54,295
這樣，在比較基礎的課程裡面，我們會想說，我們現在已經有一些

100
00:10:54,295 --> 00:10:59,890
這個很具體的特徵在手上，然後我們用這個來做學習的工作

101
00:10:59,890 --> 00:11:05,025
好，那這邊是一個，再給大家一個題目說

102
00:11:05,025 --> 00:11:14,725
我們今天如果要做一個線上的影像的這個廣告城市
那這個城市想要用機器學習的話，我們可能有哪些特徵可以用，

103
00:11:14,725 --> 00:11:19,290
具體的？還是原始的？還是抽象的？還有哪些特徵是可以用的。

104
00:11:19,290 --> 00:11:24,480
好，大家想一想以後可能會發現，誒，都有啊

105
00:11:24,480 --> 00:11:29,670
例如說具體的可能是說，我們知道有關這些使用者的資料，就像顧客資料一樣。

106
00:11:29,670 --> 00:11:38,770
那，影像裡面可能有些原始的這個pixel的資訊，那或者
我們今天可能有這些影像，這個的ID的編號等等

107
00:11:38,770 --> 00:11:46,760
這些是非常非常抽象的，在大部分現實的機器學習問題裡面，其實通常都是都有。然後怎麼樣

108
00:11:46,760 --> 00:11:54,750
由這些很多不同的X裡面，去找出一個好的表現的方式，其實，常常是機器學習

109
00:11:54,750 --> 00:11:59,080
能不能成功的關鍵所在，那我們會在未來的課程再來跟大家提到。

110
00:11:59,080 --> 00:12:06,805
好，總結來說，我們今天跟大家講了很多很多各式各樣的機器學習問題，我們其實是

111
00:12:06,805 --> 00:12:14,530
用四個方向跟大家講，一個是，如果輸出的空間不一樣的時候，我們要有哪些變化，我們說裡-
面最核心的，是

112
00:12:14,530 --> 00:12:20,600
Binary Classification還有這個regression，
就是是非題，還有會分析這兩個問題。

113
00:12:20,600 --> 00:12:29,750
那麼如果我們拿到的資料的總類，特別是標界總類
不一樣的時候有哪些變化，那我們說要裡面最基礎的是

114
00:12:29,750 --> 00:12:39,130
這個supervised，也就是我們拿到所有我們想要的輸出的資料
那如果我們今天給電腦的資料的方式不一樣，我們有哪些的變化？那我們說到說，裡面

115
00:12:39,130 --> 00:12:44,460
最重要的實際上是如果我們整批位電腦化Batch會發生什麼事。

116
00:12:44,460 --> 00:12:50,520
那如果我們輸入不一樣的時候會怎麼樣子，那我們說，最核心的是我們拿到concrete-
的時候可以怎麼做。

117
00:12:50,520 --> 00:12:56,580
那如果是其他比較複雜的例子的話，都可能需要找出一些

118
00:12:56,580 --> 00:13:00,665
更concrete的特徵之後，劃歸到concrete的例子來做。

119
00:13:00,665 --> 00:13:05,840
好，這是我們今天跟大家講的各式各樣不同的學習方式，那我們在這裡

120
00:13:05,840 --> 00:13:11,360
就已經告訴大家，好，機器學習的整個流程

121
00:13:11,360 --> 00:13:21,020
還有各種不同面向，以大家現在對於想要做是非題這樣的東西，已經有
很具體的瞭解。在下一次的上課

122
00:13:21,020 --> 00:13:30,020
怎樣埋一個梗在這邊，我們可能會先跟大家講一個壞消息；
這個壞消息是，機器學習搞不好是做不到的。

123
00:13:30,020 --> 00:13:35,880
你說，唉，怎麼可能？老師你要上八個禮拜的課，然後呢，你第二個禮拜就跟我說機器學習是-
做不到？

124
00:13:35,880 --> 00:13:40,955
好，我們下個禮拜會討論，下一次上課會討論說，機器學習為什麼

125
00:13:40,955 --> 00:13:46,030
可能是做不到的，那我們再來看看，我們能不能解決這件事。

126
00:13:46,030 --> 00:13:51,330
好，所以下一次，有個心理準備，我們要跟大家講一些壞的消息。

127
00:13:51,330 --> 00:13:57,215
好，今天謝謝大家，那我們下次見。

128
00:13:57,215 --> 00:14:03,100
【音樂】

129
00:14:03,100 --> 00:14:08,325
【音樂】

