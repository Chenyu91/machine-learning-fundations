1
00:00:00,000 --> 00:00:05,970
好，那我們現在講完了這個機器學習的這個完整的流程

2
00:00:05,970 --> 00:00:13,190
那最後一點時間我想要跟大家釐清一下機器學習跟大家可能聽過的一些相關領域的關係

3
00:00:13,190 --> 00:00:18,404
我們要講的第一個領域就是資料探勘，大家剛聽到這個KDDCUP。

4
00:00:18,404 --> 00:00:26,753
KDDCUP實際上是一個
資料探勘屆的比賽。那你說：”老師你爲什麽又說它是機器學習界最重要的比賽？”

5
00:00:26,753 --> 00:00:34,505
那我們來看看資料探勘
跟機器學習有什麽不一樣。我們剛對機器學習定義是這樣。機器學習是我們希望用資料

6
00:00:34,505 --> 00:00:40,720
去找出一個，這個hypothesis, 這個假說區。然後它跟這個我們想要的這個

7
00:00:40,720 --> 00:00:44,890
目標f 很相像，資料探勘

8
00:00:44,890 --> 00:00:51,850
做的是什麽事情。一個簡單的定義是： 資料探勘希望能夠用資料去找出一些

9
00:00:51,850 --> 00:01:00,900
有趣的事情。你說這樣講很籠統，什麽有趣的事情？
比如說如果你今天是個賣東西的人，在超市你是一個超市的經營者

10
00:01:00,900 --> 00:01:06,880
你可能會想 到底一般人如果買了這個東西的話，會不會也想要買另外一個東西

11
00:01:06,880 --> 00:01:14,096
所以你有一堆超級市場的銷售資料，你可能想要知道說
有沒有哪些東西彼此之間是有關聯性的。

12
00:01:14,096 --> 00:01:21,225
這就是個有趣的 資料裏面有趣或有用的地方。那資料探勘在傳統上通常使用非常大量的

13
00:01:21,225 --> 00:01:27,230
資料。然後試圖找出對特定的應用有趣的或有用的一些性質

14
00:01:27,230 --> 00:01:36,873
所以從這兩個定義大家可以看出來，如果有用的性質就是直接找出一個hypothesis
直接找出一個區，

15
00:01:36,873 --> 00:01:43,520
讓我們可以拿來做這些預測等等 那資料探勘跟機器學習其實沒什麽不一樣。

16
00:01:43,520 --> 00:01:52,740
它們目標是一致的 如果，這是個很大的如果，因爲資料探勘裏面有一些問題它並不是這樣

17
00:01:52,740 --> 00:02:00,560
也許它在意的不是預測。它在意的只是找出來以後能不能幫助人進一步來分析這個問題

18
00:02:00,560 --> 00:02:06,520
所以，例如說我們剛才講的KDDCup這樣的比賽上面

19
00:02:06,520 --> 00:02:12,430
它定義的問題通常是要你設計一個研算法，然後去增進

20
00:02:12,430 --> 00:02:18,340
某一種表現。也就是去趨進某一個我們理想上的f

21
00:02:18,340 --> 00:02:27,695
所以在KDDCup這樣的比賽上，通常 資料探勘跟機器學習沒什麽不一樣

22
00:02:27,695 --> 00:02:35,680
好，那這個這個，如果今天 有趣的這個性質，資料才能想要找出來的性質

23
00:02:35,680 --> 00:02:44,740
跟機學習想要做成諮詢，想要找出好的hypothesis
有點關係。例如說，你如果找出這個性質，搞不好你就可以找出更好的hypothesis。

24
00:02:44,740 --> 00:02:51,600
那麽，資料探勘跟機器學習，就可以互相幫忙。

25
00:02:51,600 --> 00:02:56,030
也就說，你可以用資料探勘工具來幫助你機器學習做得更好。

26
00:02:56,030 --> 00:03:00,790
或你可以用機器學習的工具來幫助你在資料探勘裏面找出有趣的東西。

27
00:03:00,790 --> 00:03:05,480
這點非常的常見，不是總是這樣，但是非常的常見。

28
00:03:05,480 --> 00:03:13,430
好，所以呢這是資料探勘和機器學習非常地相像。那在傳統上的資料探勘呢

29
00:03:13,430 --> 00:03:18,820
通常還有另外一個重點，是它們希望

30
00:03:18,820 --> 00:03:24,210
在很多很多的資料，特別是在資料庫裏面的資料還能够很有效率地計算

31
00:03:24,210 --> 00:03:33,990
這是傳統資料探勘從資料庫這邊切入的一個領域，好所以這是機器學習跟資料探勘
的一些關係。他們非常相像，有一些些不一樣

32
00:03:33,990 --> 00:03:40,595
好，現代，說像我吧，我的研究領域是機器學習，但是你說

33
00:03:40,595 --> 00:03:47,200
我有沒有做資料探勘，我兩個也有。所以現在來說你要找到一個研究者說：我只做機器學習，-
不做資料探勘。

34
00:03:47,200 --> 00:03:55,225
或我只做資料探勘，不做機器學習。那幾乎是不可能的事 這兩個領域非常的密不可分

35
00:03:55,225 --> 00:04:04,080
那另外一個相關的領域是人工智慧。大家說機器學習跟人工智慧有什麽關係？
我們現在知道機器學習的定義，我們來看人工智慧的定義

36
00:04:04,080 --> 00:04:10,524
人工智慧的定義是：我們希望電腦作出一個 某種東西。我們叫something。

37
00:04:10,524 --> 00:04:16,340
這個something要什麼呢，這個something要 shows
intelligent behavior。

38
00:04:16,340 --> 00:04:24,060
就是說它要有一些聰明的表現，例如說電腦會
自動下棋，這是很聰明的一件事。或者呢，電腦會自動開車，這是很聰明的一件事。

39
00:04:24,060 --> 00:04:33,672
好，那會預測 是很聰明的一件事，沒有錯。機器學習說我們要找出一個區
這個區跟我們想像的f

40
00:04:33,672 --> 00:04:42,990
很接近，你覺得會預測。會預測是一個
很聰明的事情，沒有錯。所以從這個角度出發，我們可以看成機器學習是實現

41
00:04:42,990 --> 00:04:51,270
人工智慧的一種方法。人工智慧有很多 方法可以實現，那機器學習是實現人工智慧的一種方法

42
00:04:51,270 --> 00:05:00,007
但是還有什麽不同方法，例如說大家想像下棋這個問題好了
傳統人工智慧在解下棋這個問題，常常會使用到例如說：

43
00:05:00,007 --> 00:05:06,060
好，今天我這個下棋 下這一步的好處壞處的這個分析很像一個樹狀圖

44
00:05:06,060 --> 00:05:12,050
展下去。一般叫做 Game
Tree。這是傳統人工智慧在解決棋類問題的時候可能會用的方法。

45
00:05:12,050 --> 00:05:20,760
那機器學習可能是一個不同的方法。例如說什麽？
我們秀給機器看說，今天有這麽多的棋手，他們是這樣下棋

46
00:05:20,760 --> 00:05:24,830
或者讓機器自己去下棋，下一下說： 這樣下會贏，這樣下會輸

47
00:05:24,830 --> 00:05:29,820
然後從這些資料裏面，讓機器自己去分析，最後決定要怎麽樣下棋。

48
00:05:29,820 --> 00:05:35,475
所以看兩個不同的方式，我們設計的算法讓機器去分析這個樹狀圖。

49
00:05:35,475 --> 00:05:41,130
這是一種方式。我們設計一種算法，讓機器去從資料裏面學到怎樣下棋，這是另外一種方式。

50
00:05:41,130 --> 00:05:47,160
好，那這個機器學習是實現人工智慧的一種方式。

51
00:05:47,160 --> 00:05:52,030
第三個有關係的領域我想要跟大家講一講

52
00:05:52,030 --> 00:06:00,540
是統計。機器學習跟統計的關係是什麽？好，兩個都使用資料
那統計想要使用資料來做一些推論

53
00:06:00,540 --> 00:06:07,710
推論什麽？推論一個我們原來不知道的事情，例如說：丟銅板，丟銅板，銅板的正面的機率是-
多少？我們本來不知道

54
00:06:07,710 --> 00:06:14,880
我們丟了1000次以後，我們去估計一下，推論一下說這個銅板正面的機率是多少？
這是統計要做的事情。

55
00:06:14,880 --> 00:06:19,575
好，那我們可以想像： g 我們想要

56
00:06:19,575 --> 00:06:29,098
的這個假說區實際上是一個推論的結果 然後呢，我們想要的那個這個目標

57
00:06:29,098 --> 00:06:34,320
f 實際上是一個我們不知道的事情。這在我們剛才設定裏面都有

58
00:06:34,320 --> 00:06:41,050
所以從這個角度來說，統計實際上是實現機器學習的一種方法。

59
00:06:41,050 --> 00:06:48,160
對不對？我們只要說 f 是我們不知道的那個，g 是我們想要從資料推論

60
00:06:48,160 --> 00:06:53,760
出來的東西。我們就可以用統計的工具來實現機器學習。

61
00:06:53,760 --> 00:06:59,870
所以呢，傳統的統計很多工具會用在機器學習上面。

62
00:06:59,870 --> 00:07:06,890
但傳統統計學是從數學出發的。所以你會看到統計學裏面，很多東西它會想辦法

63
00:07:06,890 --> 00:07:14,410
寫下一些假設，然後最後可以證明的結果說：在這樣的狀況下，在這樣的

64
00:07:14,410 --> 00:07:24,350
統計的數量之下，我們可以有怎麽樣子的
可以證明的推論。傳統統計學，比較很多是數學上面的推論

65
00:07:24,350 --> 00:07:32,150
但是機器學習的話，它是從電腦科學，從資料等等，從資料庫計算等等這些出發的

66
00:07:32,150 --> 00:07:38,110
所以機器學習裏面常常會有很多演算法是，更重視怎樣算出來

67
00:07:38,110 --> 00:07:44,070
不只是數學上的結果怎麽樣。好，這是一些方向上的細緻的差異

68
00:07:44,070 --> 00:07:53,490
總總來說，我們其實之後會學到
我們在機器學習裏面用的很多工具，其實有的很早很早在統計學裏面就有。

69
00:07:53,490 --> 00:08:00,870
只是，我們從統計學借過來，然後可以用機器學習的角度來看看，看看說這些工具

70
00:08:00,870 --> 00:08:03,656
對機器學習有什麽樣的幫助。

71
00:08:03,656 --> 00:08:08,830
好，所以又到這個了讓大家看一看這個問題的時間了。

72
00:08:08,830 --> 00:08:16,860
那我這邊以下列了4個句子 請大家看一看說，這4個句子

73
00:08:16,860 --> 00:08:26,610
哪些是對的，然後哪一個可能沒有那麽正確 好大家看一看，我希望大家能夠選出正確的答案

74
00:08:26,610 --> 00:08:31,550
我們建議的答案是3。那這個3原來句子是說

75
00:08:31,550 --> 00:08:37,250
資料探勘和機器學習是一模一樣。可是我們剛才有點花力氣，跟大家講說

76
00:08:37,250 --> 00:08:46,950
資料探勘和機器學習非常非常接近，但是它們的取向可能有些不同，然後有些細緻的差別
所以我們其實還是很難把它們說是一模一樣的東西

77
00:08:46,950 --> 00:08:55,580
好 那總結來說，我們今天跟大家講的這個，

78
00:08:55,580 --> 00:09:05,530
機器學習這個問題。我們從這個課的介紹出發，然後說我們
這個課會從基礎切入，並且呢，像說故事一樣。

79
00:09:05,530 --> 00:09:10,910
那什麽是機器學習呢？我希望大家現在了解，機器學習實際上是從資料出發，然後

80
00:09:10,910 --> 00:09:16,290
我們想要找到一個函數，這個函數跟我們最渴望的目標是很接近的。

81
00:09:16,290 --> 00:09:20,760
那機器學習在很多很多地方都有所應用。

82
00:09:20,760 --> 00:09:25,718
那機器學習裏面核心我們說最重要的演算法 A 資料 D

83
00:09:25,718 --> 00:09:32,050
然後呢這個hypothesis set H 然後我們最後要得到的hypothesis

84
00:09:32,050 --> 00:09:38,625
G 那機器學習跟其他不同領域的關係，跟資料探勘也好

85
00:09:38,625 --> 00:09:45,200
人工智慧也好，統計也好。它們其實有很多的關係，但是各自有各自不一樣的取向。

86
00:09:45,200 --> 00:09:53,271
那我們在下一堂課我們會開始跟大家講說，到底我們剛講機器學習
的演算法，機器學習的hypothesis

87
00:09:53,271 --> 00:10:02,020
set，以及學習的整個模型，我們會跟大家講一個這個
很重要，很簡單但很重要的一個模型。那請大家下一次我們再回來。

88
00:10:02,020 --> 00:10:08,955
[配樂]

89
00:10:08,955 --> 00:10:15,890
[配樂]

90
00:10:15,890 --> 00:10:21,890
[配樂]

