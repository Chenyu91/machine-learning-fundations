1
00:00:00,000 --> 00:00:08,850
好，那講完了這個各種在輸出空間上做的變化， 我們再來講一些這個機器學習不一樣的變形，

2
00:00:08,850 --> 00:00:18,320
我們現在呢或從什麽出發，我們從，我們已經會的這個
多類別的這個問題說，我們要分辨各種不一樣的銅板出發。

3
00:00:18,320 --> 00:00:26,320
好，我們之前想象的機器學習的方法是怎麼樣。我們說，來我們把收集了一堆銅板，就是圖上

4
00:00:26,320 --> 00:00:35,140
這四個不同顏色的這些點，我們收集這些銅板，我們把他們的大小，
我們把他們的重量都收集好，然後把這些資料，連同這些銅板是

5
00:00:35,140 --> 00:00:41,055
什麽意思，然後我們就喂給機器學習的演算法，然後說，好，你在裏面去找

6
00:00:41,055 --> 00:00:46,970
出來說，怎麼樣分這四個不同的類別，給我一個G，它最後可以分這四個不同的類別。

7
00:00:46,970 --> 00:00:53,680
好，那這樣的問題，我們一般叫做supervised
learning，一般翻譯成監督式的學習，

8
00:00:53,680 --> 00:01:01,830
什麽叫監督，我們用什麽監督，實際上監督的意思就是老師，
老師教了什麽，老師教了說，我給了這些銅板

9
00:01:01,830 --> 00:01:10,150
之後，還要我給什麽，我給他們正確是意思是什麽， 這是一顆10
cent的銅板，這是一顆1 cent的銅板，這是一顆什麽。

10
00:01:10,150 --> 00:01:15,130
我通通都告訴我們的學生，所以這是一個比較完整的教學，我給你銅板，

11
00:01:15,130 --> 00:01:20,610
我還告訴你銅板的意思是什麽。那這種，我們叫做supervised
learning，監督似的學習。

12
00:01:20,610 --> 00:01:26,385
好，那如果今天，我不告訴學生

13
00:01:26,385 --> 00:01:32,160
說銅板的意思是什麽，我可不可以學到東西呢，我們來看，如果我不告訴你，銅板的意思-
是什麽，

14
00:01:32,160 --> 00:01:37,400
如果你是機械化，你看到的差不多就是一個這樣的圖。

15
00:01:37,400 --> 00:01:43,220
好，從這張圖來看呢，你可能也可以看到說，好，所以

16
00:01:43,220 --> 00:01:49,040
銅板並不是這個肆肆散散零零落落在我們這個平面上的，它還是有一些規則，

17
00:01:49,040 --> 00:01:54,680
好，左邊這邊有一些這個比較小，比較輕的銅板，右邊這邊有一些比較大，比較重的銅板，

18
00:01:54,680 --> 00:02:01,490
好，那我能不能看看這些銅板，然後決定說，把它歸成一類一類，

19
00:02:01,490 --> 00:02:05,850
一群一群。跟剛才類別做區別，也許我們就分成一群一群的，

20
00:02:05,850 --> 00:02:11,520
譬如說，你可能說我們這個圖看起來，搞不好這裡面有

21
00:02:11,520 --> 00:02:20,580
三群不一樣的銅板。好，哪三群呢？好，我用這個圈圈來代表，
好，右上角一群，中間這邊一群，然後左下角這邊一群。

22
00:02:20,580 --> 00:02:28,067
好，這樣子的方式，把我沒有告訴電腦說，哪一個是 什麽意思，是10

23
00:02:28,067 --> 00:02:33,020
cent還是1 cent還是哪一個類別，沒有，我只是告訴它說我有這些銅板在手上，

24
00:02:33,020 --> 00:02:37,020
它自己想辦法去把它分成一堆一堆一堆的。

25
00:02:37,020 --> 00:02:44,230
那這樣的問題，我們通常叫做Clustering，分群問題，那或者我們可以把它想象-
成是一個

26
00:02:44,230 --> 00:02:50,256
Unsupervised
Multiclass，什麼樣的Unsupervised，就是我今天要不給答案了，我-

27
00:02:50,256 --> 00:02:57,022
這個不當老師了，就好像說，我把這些
銅板丟給小朋友，小朋友自己去玩，自己去決定要怎麼分群，而並不是真的教他，說這是1

28
00:02:57,022 --> 00:03:05,020
0cent，這是1 cent，這是5 cent，所以
不當老師，這就是Supervised跟Unsupervised之間，相對應的關係，

29
00:03:05,020 --> 00:03:14,120
那大家看這個例子就知道，我們做得到，做不到分群，某種角度可以，
人類也常常做這些分群的動作，就是說我們不知道這個

30
00:03:14,120 --> 00:03:19,230
完整的一些這個標記，就是說label是怎麼樣子，

31
00:03:19,230 --> 00:03:24,340
但是我們能不能好比說一些文章，或者人，或者什麽，分成一群一群，可以，

32
00:03:24,340 --> 00:03:32,462
但是分群可能會錯，對不對，在這個例子裏面，大家已經心裡
知道，我們其實有四群，四個不同的類別10 cent， 1

33
00:03:32,462 --> 00:03:39,959
cent，5 cent，25 cent，
但是呢如果，我再看這個圖的話，搞不好我覺得這世界只有三群。因為10

34
00:03:39,959 --> 00:03:48,850
cent跟1 cent長得
沒有差那麼多。所以分群呢，是一個比可能原來更困難的問題，原來分類我們至少知道有四種，

35
00:03:48,850 --> 00:03:58,350
現在我們連幾種都不知道，然後還要在裏面畫圈圈。又來多一個人搞不好說，
這裡面同一群，那也對啊，沒有什麽不對，所以分群的問題，

36
00:03:58,350 --> 00:04:02,170
要怎麼衡量，分群的好壞，通常更為困難一些。

37
00:04:02,170 --> 00:04:10,170
好的，分群這個問題，也有很多的應用，比如我剛才講的，你可以這個網絡上一堆文章，那是-
不是電腦可以自動的

38
00:04:10,170 --> 00:04:19,320
把這些文章歸到不同的這個主題裏面去。或者一個
商業公司，它有很多的顧客的資料，我們能不能把這些顧客

39
00:04:19,320 --> 00:04:25,905
分成不同的群，然後針對這些不同的群，再做不同的促銷活動。好，這些都是典型的

40
00:04:25,905 --> 00:04:32,490
分群問題，那但是，它比這個監督式的，像我們之前說的這個銅板的辨識這個問題，

41
00:04:32,490 --> 00:04:42,400
可能就要更困難一些。好，那 從這個分群問題呢，我們就引到說，所以看起來，

42
00:04:42,400 --> 00:04:51,870
雖然我們一開始說，我們的機器學習的設定是我們要拿到
資料，資料裏面有X，N，Y，就是輸入，還有我們希望的輸出的部份。

43
00:04:51,870 --> 00:04:58,500
但現在看起來好像，就算不用輸出，也許我們也可以設計一些演算法來學到一些東西，

44
00:04:58,500 --> 00:05:05,210
像是什麽。像是分群，分群就有點像是我們原來這個多類別分類問題

45
00:05:05,210 --> 00:05:15,135
的一個完全不告訴他，這個我想要輸出什麽，但是
電腦自己去做出來的這個例子，又或者是什麽，好，我今天，我給電腦

46
00:05:15,135 --> 00:05:21,020
一群這個點， 那我說電腦可不可以告訴我說，這些點在哪些

47
00:05:21,020 --> 00:05:30,310
這個地方是比較稠密的，哪些地方是比較稀疏的，或者如果我們用一些
密度函數，或者類似幾率函數來衡量的話，哪邊是

48
00:05:30,310 --> 00:05:39,930
比較密，哪邊是比較疏的。這就可以 用在什麽地方，好，例如說，你今天
喂給機器的資料是一堆，各別不同路口的這個肇事的記錄，

49
00:05:39,930 --> 00:05:48,520
你說到底哪裡比較常發生事故，哪裡比較不常 發生事故，這就是一個典型的說，這個密度分析

50
00:05:48,520 --> 00:05:56,160
的問題。那有一點點像，對應到什麽，對應到我們在監督式學習裏面，Ondate
Regression就是

51
00:05:56,160 --> 00:06:04,270
有範圍的回歸分析的問題，那現在只是，我們沒有 Y，但是我們想要這個學到說，下面潛藏

52
00:06:04,270 --> 00:06:10,000
的這個Y ，假設這個Y有點像是密度函數或者是幾率函數，怎麼樣才將這些細節

53
00:06:10,000 --> 00:06:16,810
更加函數化，它會長什麼樣子，好，又或者，一個典型的這個Unsupervised-
非監督式

54
00:06:16,810 --> 00:06:23,620
的應用，是我們叫做outlier
detection，也就是說什麽，我今天有一堆資料，這些資料大部份是正常的，

55
00:06:23,620 --> 00:06:30,065
有小部份怪怪的，例如說如果今天是一個網路，流量的資料的話，怪怪的

56
00:06:30,065 --> 00:06:36,510
可能代表入行者，或者是代表某一個機器不正常的運作，那我們不能自動的把它

57
00:06:36,510 --> 00:06:42,738
找出來，那這樣的問題有像什麽，有點像一個是非題，只是說今天的那個是，好像就是有異常-

58
00:06:42,738 --> 00:06:51,695
的情形，非常非常的少，
所以它是這個非常極端的一個是非題，那一般這樣的，可以用非監督式的方式，因為

59
00:06:51,695 --> 00:06:58,980
我們說實在話，因為那些這個異常的狀況很少，所以我們可能也很難取得有標記的資料。或我-
們可能也這樣在那邊跑，

60
00:06:58,980 --> 00:07:03,490
然後跑一跑，它突然發現，這一筆不太對，它會自動的去取出來。

61
00:07:03,490 --> 00:07:11,810
好，我舉出這些非監督式學習的例子，想要告訴大家什麽，
大家發現，它們的目標，其實還蠻分散的，

62
00:07:11,810 --> 00:07:19,910
而監督式，監督式因為我們有這些固定的
這些標記，這些label在那邊，所以我們大概知道，我們的目標是什麽，

63
00:07:19,910 --> 00:07:29,467
但是非監督式的話，目標比較分散，通常你也比較難衡量演算法
的好壞，那它有非常非常多，各式各樣不同的演算法，

64
00:07:29,467 --> 00:07:35,560
那這些演算法 很多會對應到監督式學習的一些演算法的概念。

65
00:07:35,560 --> 00:07:40,420
好，講完了監督式跟非監督式，

66
00:07:40,420 --> 00:07:46,900
有一些比較聰明的同學可能就想，那我能不能有一個中間的情形，中間的情形什麽意思，好，

67
00:07:46,900 --> 00:07:52,257
我一樣給了一堆點，有的點，少數的點， 我告訴他這是10

68
00:07:52,257 --> 00:08:00,360
cent，還是1 cent，還是5 cent，還是25
cent，我告訴他， 等大多數的點，我不告訴他，等於說我就是舉幾個例子，

69
00:08:00,360 --> 00:08:10,130
其他的，我還是維持著說自己去找到
答案吧，非監督式就想自己去找到答案，監督式就是你就每個例子都告訴他

70
00:08:10,130 --> 00:08:15,965
標準答案，好，這樣的問題一般我們叫做semi-supervised
一般叫做半監督式的問題，

71
00:08:15,965 --> 00:08:21,800
可以用在什麽地方，好，例如說，今天有很多的照片。在網路上，

72
00:08:21,800 --> 00:08:29,110
譬如說臉的照片在網路上好了，那你說真的要大家都很用力的去把每一個照片的這個答案-
找出來，

73
00:08:29,110 --> 00:08:36,990
太困難了，這是，也就是說我們要取得 這些照片後面的答案，這些Y，是很花

74
00:08:36,990 --> 00:08:41,100
力氣的事情，可能只能標出少部份的照片而已。

75
00:08:41,100 --> 00:08:46,715
不過少部份的照片，配合上很多還沒有標記的照片，也許，

76
00:08:46,715 --> 00:08:52,330
機器可以學習到說，那到底這些還沒有標記的裏面，哪一個是代表這個iii，

77
00:08:52,330 --> 00:08:59,935
iii的照片可能只標兩張，而還有很多其他沒有標的，機器可以自動的去學習到，有標的照-
片對應到這，

78
00:08:59,935 --> 00:09:07,540
中間的這些彩色的點，沒有標的照片，就對應到這些藍色的點。機器可以用這些沒有標的跟有-
標的混起來，

79
00:09:07,540 --> 00:09:13,670
然後學的更好，好或者是，今天是藥的資料，今天我們可以做出很多

80
00:09:13,670 --> 00:09:19,800
我們想要的藥，醫藥裏面這個藥的資料，但是我們只能，

81
00:09:19,800 --> 00:09:29,060
做測試很貴，不管做動物測試或人體測試都很貴，所以只有少
部份的藥，我們能夠做測試，然後得到說，也許它能夠有效，還是沒有效，

82
00:09:29,060 --> 00:09:34,850
那有很多的各種不同的藥的可能性，我們有少數有標記的結構

83
00:09:34,850 --> 00:09:40,640
說它有效還沒有效，這些資料混起來，可能機器就告訴我們說，到底

84
00:09:40,640 --> 00:09:47,460
哪一邊，那一類的藥是有效的，哪一類的藥是沒有效的，這些都是典型的半監督式的問題，

85
00:09:47,460 --> 00:09:51,250
大家看到這兩個問題裏面有一個特點，特點實際上就是

86
00:09:51,250 --> 00:09:58,275
要找到標記很貴，你要知道人做下來，一直跟你標說，這個點是誰，這個點是誰。

87
00:09:58,275 --> 00:10:05,300
標識上他就不做了。或者要你要去做測試也許要花很多時間，也許要花很多錢，在這個狀況下

88
00:10:05,300 --> 00:10:13,300
可能就要採用半監督式這個學習方式。好那這個半監督式的學習方式實際上希望的一件事情

89
00:10:13,300 --> 00:10:23,110
就是有點像監督式，但是我們要用很大量的
這個還沒有標的資料來進一步地讓機器學習的表現更加提升。

90
00:10:23,110 --> 00:10:32,870
那很多半監督式學習演算法實際上也都用到在我們
在設計監督式學習supervised的演算法的時候的一些概念來做設計。

91
00:10:32,870 --> 00:10:39,470
好，那講完了這個機器學習裡面最基本的三個，

92
00:10:39,470 --> 00:10:46,740
監督式，非監督式還有半監督式，我現在想要跟大家講一個在機器學習裡面不一樣的

93
00:10:46,740 --> 00:10:52,515
這個學習的方法。大家等一下會聽到它們非常不一樣的跟我們剛才講的說

94
00:10:52,515 --> 00:10:58,290
我把資料標好然後送去然後就學，或者我偷懶我資料就偷偷不標然後去學完全不一樣。

95
00:10:58,290 --> 00:11:06,540
這個方法是什麼呢？我希望大家想一下，如果你有養
寵物或者類似的東西的話，你怎麼訓練你的寵物的。

96
00:11:06,540 --> 00:11:11,560
比如說你怎麼訓練你的寵物說，你跟它說坐下，它就坐下。

97
00:11:11,560 --> 00:11:20,300
這是一個典型的方法好你說的坐下 寵物可能沒有聽懂或者還未經訓練。

98
00:11:20,300 --> 00:11:25,000
結果呢它聽到坐下，它就在地上尿尿了。

99
00:11:25,000 --> 00:11:32,020
在地上，你說這不對阿。你要它坐下，它怎麼在地上這個這個隨地小便？

100
00:11:32,020 --> 00:11:38,215
這是不正確的行為，所以你就你也許罵它，你也許懲罰它。你用任何

101
00:11:38,215 --> 00:11:44,410
你想的到然後它會覺得不開心的方法去懲罰它。你說這是一個非常錯誤的

102
00:11:44,410 --> 00:11:50,940
事情。好所以你想辦法教它。在這個例子裡面，你想要它坐下，你也說了坐下。

103
00:11:50,940 --> 00:11:58,790
你有沒有辦法真正跟它說來你坐下，然後這是一個正確的動作，還是說你示範給它看要坐下還-
是怎麼樣？

104
00:11:58,790 --> 00:12:05,420
通常很困難，你並沒有辦法直接跟它說，說你在送給它坐下這個指令

105
00:12:05,420 --> 00:12:12,050
也就是x的時候你希望的輸出是真正坐下那個類別那個y。

106
00:12:12,050 --> 00:12:16,420
對吧？這不是一個你直接可以告訴你的寵物的資料。

107
00:12:16,420 --> 00:12:24,570
但是呢透過懲罰，你可以讓它知道什麼？ 你可以讓它知道它做了一個錯誤的

108
00:12:24,570 --> 00:12:29,825
判斷。錯誤的判斷是什麼？它錯誤的判斷就是，它判斷說你在教它說

109
00:12:29,825 --> 00:12:35,080
要上廁所。不對啊，你說這個不對，你要的是坐下。

110
00:12:35,080 --> 00:12:41,510
你沒有辦法說直接坐下是對的，但是你可以說這個它選擇的另外一個動作是錯的。

111
00:12:41,510 --> 00:12:48,940
好，你是，你就這樣訓練。然後呢，下一次你又說坐下的時候，你的狗狗好像聽懂了。

112
00:12:48,940 --> 00:12:58,190
聽懂它就乖乖坐下。你就說哦好好好好棒啊，那
我給你餅乾吃，或者我給你這個任何它有興趣的零食吃。

113
00:12:58,190 --> 00:13:03,740
在這個例子你給它餅乾吃是不是真的說它做到了最正確的

114
00:13:03,740 --> 00:13:09,335
事情？也不盡然，你只是說這個動作好而已。搞不好至少如果你說坐下，它說握手

115
00:13:09,335 --> 00:13:14,930
你還是很高興，你還是給它餅乾吃。好這就是說並不是你最期待的那個坐下的那個指令。

116
00:13:14,930 --> 00:13:24,060
所以在這裏呢，你是用什麼樣的方式 來讓你的寵物學到東西呢？你用的方式是用

117
00:13:24,060 --> 00:13:32,060
獎勵或者用懲罰。獎勵懲罰在什麼身上？在另外不一定你原先想要

118
00:13:32,060 --> 00:13:40,790
輸出，而是在另外一個輸出上面你說這個輸出好，還是這個
輸出不好，你用一些方式來告訴你的寵物。

119
00:13:40,790 --> 00:13:47,030
今天做機器學習你也可以用這樣的方式。那這樣的方式我們通常叫作reinforce-
ment learning。

120
00:13:47,030 --> 00:13:53,020
就是說真的就是像訓練你的寵物一樣來讓它們學到東西。

121
00:13:53,020 --> 00:13:58,160
那非常這個technical的speaking的話，我們可以看到說reinforc-
ement

122
00:13:58,160 --> 00:14:05,050
learning 做得事情其實是你喂給機器的資料是輸入，輸入像我們剛才的例子

123
00:14:05,050 --> 00:14:11,035
輸入就是你說的話。你有輸出，但是這個輸出並不是你真正想要輸出，而是說

124
00:14:11,035 --> 00:14:17,020
好你的寵物做了一些動作或者一些其它的輸出，然後你告訴它這個輸出

125
00:14:17,020 --> 00:14:24,530
好還是不好，你用這個獎勵或者懲罰的方式來跟它說這個輸出好還是不好。

126
00:14:24,530 --> 00:14:29,135
什麼地方會用到？好比如說現在有個線上廣告系統。

127
00:14:29,135 --> 00:14:37,620
線上廣告系統有點像什麼？有點想你的顧客在訓練這個廣告系統
怎麼做得更好。好所以你的輸入是顧客的資料。

128
00:14:37,620 --> 00:14:45,840
然後呢這個廣告系統可能就選了說我放這個廣告在這裏，
所以這是一個可能的輸出，這是那個y~。

129
00:14:45,840 --> 00:14:53,440
就是那個可能的輸出。然後顧客有沒有點，或者說他有沒有讓你賺到錢？這是放這個廣告

130
00:14:53,440 --> 00:15:00,450
好或不好，經過這個過程也許最後這個廣告系統就學到怎麼樣放最適合的廣告。

131
00:15:00,450 --> 00:15:07,215
又或者今天是一個玩牌或者玩棋類的遊戲。今天在某一個手上拿著某一副

132
00:15:07,215 --> 00:15:13,980
牌的狀況下，好這個就要決定他要這樣打。比如說你今天是玩21點。他決定他不要

133
00:15:13,980 --> 00:15:20,255
補牌了。不要補牌了以後他可能輸了或者他可能贏了。於是這是代表什麼？不要補牌這個

134
00:15:20,255 --> 00:15:26,530
行為好還是不好。是不是代表補牌是最好或不補牌是最好？不是，它只代表說這個好還是不好。

135
00:15:26,530 --> 00:15:35,751
然後也許逐漸地這個演算法就會學到怎麼樣玩
牌會越玩越好。這是所謂的reinforcement

136
00:15:35,751 --> 00:15:44,210
learning的這個設定。那它跟其它前面不一樣的地方是
我們靠的並不是很直接的說我們要怎麼樣

137
00:15:44,210 --> 00:15:53,860
輸出資訊。而是靠的另外一個輸出，還有另外這個輸出
好或不好來做學習的動作。那我們之後會提到，通常這樣學習的動作會

138
00:15:53,860 --> 00:16:01,630
序列地發生。也就是說它是一筆一筆一筆學到東西的，而不是說像我們現在想的這樣我們喂一-
堆資料給它。

139
00:16:01,630 --> 00:16:09,630
這是我們下一個課題才會提到的。總的來說我們跟大家介紹了4種不同的

140
00:16:09,630 --> 00:16:18,560
學習的方式，那這4種方式的不同在於我們在我們的 資料裡面到底有什麼樣的標記什麼樣的
yn 的資訊。

141
00:16:18,560 --> 00:16:28,150
那最普遍的是監督式的。監督式的是表示我們對所有的
輸入我們都有它對應的我們想要的輸出是什麼這樣的資訊。

142
00:16:28,150 --> 00:16:32,735
那如果我們完全沒有我們就說非監督式unsupervised，那如果我們

143
00:16:32,735 --> 00:16:41,395
只有一些些，或者一些是少量的一些些，我們會叫作semi-supervised
就是半監督式的。那如果我們沒有，但是我們有

144
00:16:41,395 --> 00:16:49,390
一些額外的輔助的資訊，也就是說因為我們實際上要
什麼？但是我們有一些其它的標記，然後這些其它的標記

145
00:16:49,390 --> 00:16:55,100
好壞是怎麼樣子的話，我們叫作reinforsement,那有的人把reinfors-
ement翻譯成增強式學習。

146
00:16:55,100 --> 00:16:58,340
也就是說我們是一步一步地讓電腦越變越強。

147
00:16:58,340 --> 00:17:05,830
那這裏還有很多種不同的這個變化的，但是我們只舉最重要的幾種能讓大家看到說在

148
00:17:05,830 --> 00:17:13,320
機器學習的基本流程裡面如果我們變化我們資料裡面有的標記資訊的話，我們也可以用各式各樣

149
00:17:13,320 --> 00:17:18,795
不同的學習方法出來，那裡面最核心的是監督式，很多的

150
00:17:18,795 --> 00:17:24,270
其它的方法都是從監督式的方法發展出來。那這也會是我們這門課的重點。

151
00:17:24,270 --> 00:17:30,845
好那在這邊我們還是給大家一個小小的測驗。這邊是說有一個公司

152
00:17:30,845 --> 00:17:37,420
它想要做樹的辨識系統，那麼之前跟大家提到過樹的辨識系統對機器學習來說是一個很典型的

153
00:17:37,420 --> 00:17:44,770
應用，那麼就收集很多的照片然後請他們公司的員工去標記一些照片裡面到底是有樹還是-
沒有樹。

154
00:17:44,770 --> 00:17:52,120
然後喂給一個機器學習的演算法，那我們要問的是這個演算法到底要解決什麼樣的問題？

155
00:17:52,120 --> 00:18:00,120
好我希望說大家看一看以後會想到說這是一個典型的半監督式的問題。我們有一些照片

156
00:18:00,120 --> 00:18:04,795
有1000張照片是有標記的，但是裡面有其它很多很多其它照片

157
00:18:04,795 --> 00:18:12,740
是沒有標記的。那這些資料通通喂給機器，我們希望能夠做得更好的話
那這是一個半監督式學習的問題。

