1
00:00:00,000 --> 00:00:04,650
所以我們證明了PLA這個演算法會停。

2
00:00:04,650 --> 00:00:09,150
甚麼意思呢?我們稍微給大家總結一下。

3
00:00:09,150 --> 00:00:18,260
我們說如果你的資料是線性可分，
然後PLA這個演算法是每一次挑一個錯出來修正，挑一個錯出來修正，

4
00:00:18,260 --> 00:00:23,190
線性可分告訴我們甚麼?告訴我們Wt跟Wf會越來越接近。

5
00:00:23,190 --> 00:00:29,740
用錯誤修正來代表甚麼?代表說我們Wt的長度會緩慢的成長。

6
00:00:29,740 --> 00:00:35,050
那麼所以我們剛才融合這兩個結果，我們證明了PLA會停下來。

7
00:00:35,050 --> 00:00:42,680
那這樣延伸的好處是甚麼？這樣延伸的好處是太簡單了！我剛說，不到二十行，對不對？然後-
它其實滿快的。

8
00:00:42,680 --> 00:00:48,645
然後我們剛才雖然都只在二維給大家看，但是大家可以想像說這裡都是向量運算，實際上你要做

9
00:00:48,645 --> 00:00:54,610
二維跟做一百維是類似的意思。只是這個五十倍的時間而已，但是整個演算法的精神

10
00:00:54,610 --> 00:01:01,310
是一樣的。它並沒有用到很特殊的幾何性質說二維要怎麼做，三維的要另外做，四維的要-
另外做。

11
00:01:01,310 --> 00:01:07,030
沒有，二維或一百維對程式來說，都是差不多的。壞處是甚麼?

12
00:01:07,030 --> 00:01:13,045
壞處是我們要先假設這個資料是線性可分。

13
00:01:13,045 --> 00:01:19,060
這是一個假設，對不對?如果說這個假設不成立的話，PLA根本就跑不完。

14
00:01:19,060 --> 00:01:24,170
但是我現在問問大家，我們知不知道這個假設對不對啊?

15
00:01:24,170 --> 00:01:30,320
我給大家的答案是不知道，為甚麼?我們這個假設是甚麼?要有Wf，

16
00:01:30,320 --> 00:01:35,580
那你如果已經知道Wf的話，你還做PLA做甚麼呢?所以我們

17
00:01:35,580 --> 00:01:40,840
要知道這個假設對不對，我們就要找一個Wf出來，這樣就陷入一個循環論證。

18
00:01:40,840 --> 00:01:49,970
所以我們一開始是不知道Wf是甚麼。既然不知道Wf是
甚麼，我們在拿到資料時候，某種角度我們是不知道PLA是會不會停下來的。

19
00:01:49,970 --> 00:01:57,820
另外，就算我們催眠我們自己說有一個Wf，那我問你，

20
00:01:57,820 --> 00:02:03,340
PLA多久會停下來?你說有啊!我剛剛推導了，R平方除以RO平方。

21
00:02:03,340 --> 00:02:07,820
R是甚麼?R是從你的資料長度算，這你可以算出來。

22
00:02:07,820 --> 00:02:14,520
RO怎麼算出來?我問大家，RO算出來是用Wf算出來的。

23
00:02:14,520 --> 00:02:24,390
Wf在哪裡?不知道。大家現在說
不知道，所以你不但不知道PLA會不會停，就算你催眠自己說PLA會停，

24
00:02:24,390 --> 00:02:32,940
你也不知道它多久會停。所以我們推導了PLA會停，但是
實務上，這部份我們是不確定，你怎麼知道PLA一直跑，

25
00:02:32,940 --> 00:02:42,236
然後停了，它就說很高興。然後如果不停的話，你就知道你搞不好，要麼你還沒跑夠，
要麼就是這個資料根本不是線性可分。所以

26
00:02:42,236 --> 00:02:47,030
PLA 真正在使用的時候是有一些麻煩的地方的。

27
00:02:47,030 --> 00:02:51,965
那如果你的資料根本不是線性可分怎麼辦?

28
00:02:51,965 --> 00:02:56,900
這是更大的問題喔！現實的資料，你怎麼知道它一定是線性可分或一定不是線性可分?

29
00:02:56,900 --> 00:03:03,315
當然要解決這個問題，我們先跟大家講說我們在機器學習的設定

30
00:03:03,315 --> 00:03:09,730
並不是真的這麼死硬的說我們拿到了資料，一定是從我們的目標

31
00:03:09,730 --> 00:03:13,790
函數很完整的這樣產生出來的。

32
00:03:13,790 --> 00:03:21,820
搞不好我們在產生資料、收集資料的過程中，有一些
雜訊，那也不一定啊。例如說我們要判斷要不要給這個人信用卡，

33
00:03:21,820 --> 00:03:30,060
那可是之前銀行給錯這個人信用卡，結果有
不好的結果。或者是說他們的理財專員怎麼樣把一些東西弄錯。

34
00:03:30,060 --> 00:03:33,560
所以我們的資料裡面可能是有雜訊的。

35
00:03:33,560 --> 00:03:38,705
那在如果用我們原來這個機器學習的整個框架好了，雜訊基本上就是

36
00:03:38,705 --> 00:03:43,850
發生在這裡。所以我們要一個目標函數，不過我們要知道我們取得資料的過程

37
00:03:43,850 --> 00:03:50,280
可能是有雜訊的。有雜訊的意思是甚麼?就算我們原來的f是一條線，

38
00:03:50,280 --> 00:03:56,730
我拿到的D，我拿到的資料也不見得會是線性可分。

39
00:03:56,730 --> 00:04:04,120
那我們現在跟大家講有沒有甚麼方法在我們不確定我們的資料是不是線性可分的狀況下，或可-
能有雜訊的狀況下，

40
00:04:04,120 --> 00:04:09,250
還是找得到一條好的線呢?好，我們來看看，

41
00:04:09,250 --> 00:04:14,260
我們還是回到一樣，我們不知道f，我們也不知道雜訊是甚麼，

42
00:04:14,260 --> 00:04:19,270
所以我們只能在我們所知道的資料上，D上面看看發生甚麼事。

43
00:04:19,270 --> 00:04:25,220
那大部分的時候我們可以假設雜訊很小，如果雜訊很大的話，

44
00:04:25,220 --> 00:04:31,170
你乾脆去學雜訊就好了，不用學f，所以雜訊相對於你真正想學的f應該是小的。

45
00:04:31,170 --> 00:04:35,920
小的狀況下代表甚麼?代表我們在我們的資料上，

46
00:04:35,920 --> 00:04:41,810
y跟f要有一定的對應程度。y跟f

47
00:04:41,810 --> 00:04:49,030
如果我把f拿去執行在X上面的話，我應該要得到大部分的時候看到的yn。

48
00:04:49,030 --> 00:04:54,500
所以如果我們要找一個g跟f很相像的話，那它在這個

49
00:04:54,500 --> 00:04:59,970
資料上面也應該要滿足甚麼?也應該要滿足說我們看到的y

50
00:04:59,970 --> 00:05:04,610
跟g預測出來的這個標籤也要非常的

51
00:05:04,610 --> 00:05:11,150
相像。也就是甚麼呢?乾脆我們就找一個甚麼?我們找一條線

52
00:05:11,150 --> 00:05:15,610
犯的錯誤最少的，所以我們現在沒有辦法找到不犯錯誤的線。

53
00:05:15,610 --> 00:05:20,070
那我們能不能夠找到一條線，犯的錯誤最少的?我們就把它叫做我們的g吧!

54
00:05:20,070 --> 00:05:28,070
所以這是退後的。其實剛才我們在PLA的時候我們說我們一定能夠找到一條完美的線，然後-
執行了一陣子後，咚，

55
00:05:28,070 --> 00:05:36,310
找到一條完美的線。現在說我們假設我們
找不到一條完美的線，那乾脆我們就找一條最好的線。那我們現在定義說最好就是

56
00:05:36,310 --> 00:05:40,550
犯錯誤在我們所看過的資料上犯錯誤最少的。

57
00:05:40,550 --> 00:05:50,060
這邊可能有一些念電腦科學的同學，念資訊的同學
我問問你們。你們會解這樣的問題嗎?我要找到

58
00:05:50,060 --> 00:05:58,450
犯錯誤最少，大家看到我用這個符號，我這邊有Y不等於
這個sign，我這邊用這個方框框代表這個布林運算，

59
00:05:58,450 --> 00:06:06,140
然後我們算說這邊是我總共犯了多少錯誤，
我在所有的可能的W裡面選一個犯錯誤最小的當作我的Wg，

60
00:06:06,140 --> 00:06:09,910
你會寫程式解決這個問題嗎?

61
00:06:09,910 --> 00:06:16,690
如果你告訴我你會，你有一個非常非常有效率的方法解決這個問題，趕快來跟我講，

62
00:06:16,690 --> 00:06:26,370
為甚麼呢?因為這個問題在電腦科學裡面已經被證明是
一個NP-hard的問題，讀電腦科學的人可能比較熟悉這個名詞是甚麼意思。

63
00:06:26,370 --> 00:06:33,740
如果不懂的人，我們就把它想成是一個很難很難很難，幾十年來電腦科學家都沒有辦法解決-
的問題。

64
00:06:33,740 --> 00:06:39,030
所以我說如果你真的有一個好的解法，趕快來跟我講。我們搞不好

65
00:06:39,030 --> 00:06:44,630
這個合作一下，然後看看能不能解決這個千古的懸題。

66
00:06:44,630 --> 00:06:54,430
不過機會大概不太大。沒有甚麼
很有效率的演算法來解決這個問題，這是我們現在不知道很有效率

67
00:06:54,430 --> 00:06:59,565
的演算法。那糟糕了!你說老師你列了一個問題，結果

68
00:06:59,565 --> 00:07:04,700
又告訴我們這個問題不能解。那這學期課是不是結束了?反正我們不能解。

69
00:07:04,700 --> 00:07:14,240
在這麼難的問題下，機器學習的科學家或以前的一些電腦科學家
就設計了一些演算法。這些演算法沒有辦法完美的解決這個問題，

70
00:07:14,240 --> 00:07:18,795
不過做得還不錯。我們現在就跟大家介紹一個簡單的演算法。

71
00:07:18,795 --> 00:07:23,350
這個演算法實際上可以看成是我們剛才學過的PLA的一個變形

72
00:07:23,350 --> 00:07:30,070
來找到一個差不多很好的線， 不見得是最完美那條線，但是差不多很好的線。

73
00:07:30,070 --> 00:07:36,920
怎麼做呢?我們的做法是一個有一點點貪心的做法。

74
00:07:36,920 --> 00:07:43,250
我們說我們剛才會PLA啊!PLA不就一直跑嗎?找到一個錯的點修正一下，找到一個錯的-
點修正一下，

75
00:07:43,250 --> 00:07:50,330
如果我們相信在這個修正的過程中，我們總會碰到一條還不錯的線的話，

76
00:07:50,330 --> 00:07:55,770
那我們可以做這樣的事情。而這個事情是有一點像小孩子，貪心的小孩子在做的。

77
00:07:55,770 --> 00:08:02,930
手上拿一個他覺得最好玩的玩具， 然後看到新的玩具，覺得那個玩具更好玩，

78
00:08:02,930 --> 00:08:08,925
把手上玩具丟掉，把新的玩具抓在手上。他一直把他覺得最好玩的玩具抓在手上。

79
00:08:08,925 --> 00:08:16,536
我們現在也是一樣，我們把什麼東西抓在手上，把我們覺得最好的那條線抓在手上
或者傳統上這個演算法叫

80
00:08:16,536 --> 00:08:24,620
pocket，pocket 是口袋的意思，
放在口袋裡面。外國人比較含蓄說放在口袋哩，我覺得其實就是抓在手上。

81
00:08:24,620 --> 00:08:33,590
所以我們做的事情是甚麼?我們就跑PLA這個
演算法。然後跟剛剛我們學的一樣，看到一個錯誤，修正一下，看到一個錯誤，修正一下，

82
00:08:33,590 --> 00:08:39,455
那通常在我們做pocket，口袋演算法的時候，我們會用

83
00:08:39,455 --> 00:08:48,720
比較隨機的更新方式。因為我們想要找到線這個多一點點
不一樣的地方。所以看看能不能找到一條比較好的。

84
00:08:48,720 --> 00:08:55,305
然後跟PLA不一樣的是甚麼?我們每一次找到一條新的線的時候，我就去看看

85
00:08:55,305 --> 00:09:01,890
這一條新的線跟我口袋裡的那一條線，哪一條比較好?甚麼叫比較好?犯錯誤比較少就是-
比較好。

86
00:09:01,890 --> 00:09:11,860
如果新的線比較好，我就把我口袋裡的丟掉。放一條新的線在
口袋裡。就這麼簡單，所以跟原來的PLA

87
00:09:11,860 --> 00:09:16,650
長得差不多。差在甚麼?我抓一條最好的線在口袋裡。

88
00:09:16,650 --> 00:09:25,580
然後這時候我們就不能確定甚麼時候停下來，
因為這不像PLA說不犯錯就停下，我們就讓它一直跑，一直跑，一直跑，

89
00:09:25,580 --> 00:09:29,380
跑到我們覺得跑得夠多了，跑得夠多代表我們看過夠多的線。

90
00:09:29,380 --> 00:09:33,440
然後可能裡面會有一條滿好的，我們再把它停下來。

91
00:09:33,440 --> 00:09:37,500
停下來的時候，我們把我們口袋裡那一條線回傳回去。

92
00:09:37,500 --> 00:09:44,580
就是這樣的這個演算法，所以它跟PLA長得非常像。大家可以看到實際上只有

93
00:09:44,580 --> 00:09:51,990
這個兩三行的不一樣而已。然後有一些理論可以證明它還可以找到不錯的線，那我們這邊不詳-
細的介紹，

94
00:09:51,990 --> 00:09:59,400
只是說我們想要告訴大家如果你是線性可分，PLA會做得很好；如果你不是線性可分，

95
00:09:59,400 --> 00:10:02,480
PLA的一個變形可能也可以做得還不錯。

96
00:10:02,480 --> 00:10:09,010
不過說到這裡，我們其實是讓同學們想一想說我們拿到一個資料，

97
00:10:09,010 --> 00:10:17,690
我們不知道它是不是線性可分。所以我們有兩個選擇，一個是我們就去跑
PLA，然後跑到天荒地老它搞不好都不會停。一個是我們

98
00:10:17,690 --> 00:10:23,085
覺得它搞不好不會線性可分，所以我們就去跑pocket，然後跑口袋演算法，

99
00:10:23,085 --> 00:10:28,480
然後跑夠多的這個執行的時間，然後看看它停下來變成甚麼。

100
00:10:28,480 --> 00:10:37,900
那我們的問題是這樣，如果
我們就去跑了口袋演算法，跑了pocket，結果這個資料真的是線性可分，

101
00:10:37,900 --> 00:10:43,145
我浪費了甚麼東西?就是說比起我假設

102
00:10:43,145 --> 00:10:48,390
一開始的時候運氣很好，我就直接決定去跑PLA，我到底

103
00:10:48,390 --> 00:10:55,720
損失了甚麼東西?我希望大家想了想以後會發現答案是

104
00:10:55,720 --> 00:11:03,050
一。就是pocket比PLA要慢。要慢有兩個原因，第一個原因是它要花力氣去存檔，

105
00:11:03,050 --> 00:11:11,000
把口袋裡面的東西存起來。另外一個實際上是它要檢查每一條線有沒有比較好?大家記得以前-
PLA的時候，我們就只要找出一個錯誤就好。

106
00:11:11,000 --> 00:11:17,400
它檢查兩條線有沒有比較好，它把所有的資料都看過一次才知道這個

107
00:11:17,400 --> 00:11:27,070
我的口袋裡面比較好，還是新的那個東西比較好。所以這個會花
額外的力氣。所以今天如果是一個線性可分的資料，你說我不要跑

108
00:11:27,070 --> 00:11:33,870
PLA，我跑pocket的話，那你會花額外的計算時間。這只是一個讓大家想一想的機會。

109
00:11:33,870 --> 00:11:40,805
今天的課程裡面我們進一步跟大家介紹是非題，我們怎樣可以讓

110
00:11:40,805 --> 00:11:47,529
電腦，我們介紹了一個線性的hypothesis set，線性的

111
00:11:47,529 --> 00:11:54,611
H 那裡面的東西實際上就是我們說高維的平面，
或者是二維上實際上就是一條線。我們叫linear

112
00:11:54,611 --> 00:12:00,394
classify，或者是
perceptron這個感知器。那我們介紹一個跟這個hypothesis

113
00:12:00,394 --> 00:12:07,590
set相對應的演算法， PLA。那但是PLA只會在這個線性可分的時候有用。

114
00:12:07,590 --> 00:12:15,445
那我們證明了它的線性可分的時候會最後會停下來。但是如果不是線性可分的時候，你可以-
把PLA

115
00:12:15,445 --> 00:12:23,300
改成pocket，然後還是可以做得還不錯。但這是希望大家對機器學習會有一個比較具體-
的認識。

116
00:12:23,300 --> 00:12:32,810
下一次的課程裡面我們會跟大家說機器學習不是只能
做是非題。還可以做很多很多不一樣的問題，或很多很多不同的資料形式。

117
00:12:32,810 --> 00:12:39,095
那是下一次課程我們要說的。好，謝謝大家！

118
00:12:39,095 --> 00:12:45,380
[音樂]

119
00:12:45,380 --> 00:12:50,585
[音樂]

120
00:12:50,585 --> 00:12:55,790
[音樂]

